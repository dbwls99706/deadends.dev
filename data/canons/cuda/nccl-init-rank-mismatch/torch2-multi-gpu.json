{
  "schema_version": "1.0.0",
  "id": "cuda/nccl-init-rank-mismatch/torch2-multi-gpu",
  "url": "https://deadends.dev/cuda/nccl-init-rank-mismatch/torch2-multi-gpu",
  "error": {
    "signature": "RuntimeError: NCCL error: invalid argument - ncclInvalidArgument",
    "regex": "NCCL error.*invalid argument|ncclInvalidArgument",
    "domain": "cuda",
    "category": "distributed_computing",
    "first_seen": "2022-01-01",
    "last_confirmed": "2026-02-14"
  },
  "environment": {
    "runtime": {
      "name": "pytorch",
      "version_range": ">=2.0,<2.5"
    },
    "os": "linux",
    "hardware": {
      "gpu": "multi-gpu",
      "interconnect": "NCCL"
    }
  },
  "verdict": {
    "resolvable": "true",
    "fix_success_rate": 0.82,
    "confidence": 0.85,
    "last_updated": "2026-02-14",
    "summary": "NCCL initialization fails because the world_size, rank, or MASTER_ADDR/MASTER_PORT environment variables are misconfigured. The ranks disagree about the total number of participants, or a rank ID is out of range."
  },
  "dead_ends": [
    {
      "action": "Setting NCCL_DEBUG=INFO without checking environment variables first",
      "why_fails": "While NCCL_DEBUG provides useful logging, the root cause is almost always that WORLD_SIZE, RANK, MASTER_ADDR, or MASTER_PORT are set incorrectly. Debug logs will just confirm the mismatch without explaining why the environment is wrong.",
      "fail_rate": 0.6,
      "sources": [
        "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html"
      ],
      "condition": ""
    },
    {
      "action": "Upgrading NCCL version to fix the 'invalid argument' error",
      "why_fails": "ncclInvalidArgument is a parameter validation error, not a bug. The communicator creation rejects the configuration because rank >= world_size or world_size doesn't match across processes.",
      "fail_rate": 0.8,
      "sources": [
        "https://docs.nvidia.com/cuda/"
      ],
      "condition": ""
    }
  ],
  "workarounds": [
    {
      "action": "Ensure WORLD_SIZE equals the actual number of processes and each RANK is unique in [0, WORLD_SIZE)",
      "how": "# For torchrun (recommended):\ntorchrun --nproc_per_node=4 --nnodes=1 train.py\n# torchrun sets RANK, WORLD_SIZE, LOCAL_RANK, MASTER_ADDR, MASTER_PORT automatically\n\n# For manual launch, verify:\n# WORLD_SIZE = total GPUs across all nodes\n# RANK = unique per-process (0 to WORLD_SIZE-1)\n# LOCAL_RANK = GPU index on this node",
      "success_rate": 0.9,
      "sources": [
        "https://pytorch.org/docs/stable/elastic/run.html"
      ],
      "condition": ""
    },
    {
      "action": "Use torchrun instead of manual dist.init_process_group() with environment variables",
      "how": "# Replace:\n# python -m torch.distributed.launch --nproc_per_node=4 train.py\n# With:\ntorchrun --nproc_per_node=4 --nnodes=1 --rdzv_backend=c10d --rdzv_endpoint=localhost:29500 train.py\n\n# In the script:\nimport torch.distributed as dist\ndist.init_process_group(backend='nccl')  # torchrun handles env vars",
      "success_rate": 0.88,
      "sources": [
        "https://pytorch.org/docs/stable/elastic/run.html"
      ],
      "condition": "Using deprecated torch.distributed.launch"
    },
    {
      "action": "For multi-node, verify MASTER_ADDR is reachable from all nodes and MASTER_PORT is open",
      "how": "# On each worker node, verify connectivity:\nnc -zv $MASTER_ADDR $MASTER_PORT\n# MASTER_ADDR should be the IP of rank 0 node, not 'localhost'\n# Ensure firewall allows the port range (default 29500)",
      "success_rate": 0.85,
      "sources": [],
      "condition": "Multi-node distributed training"
    }
  ],
  "transition_graph": {
    "leads_to": [
      {
        "error_id": "cuda/nccl-timeout/torch2.1-multi-gpu",
        "probability": 0.3,
        "condition": "After fixing rank mismatch, NCCL may timeout if network connectivity between nodes is poor"
      }
    ],
    "preceded_by": [],
    "frequently_confused_with": [
      {
        "error_id": "cuda/nccl-timeout/torch2.1-multi-gpu",
        "distinction": "ncclInvalidArgument fails immediately during init_process_group due to wrong rank/world_size configuration. NCCL timeout hangs for minutes then fails, indicating network connectivity or synchronization issues between correctly-configured ranks."
      }
    ]
  },
  "metadata": {
    "generated_by": "expert_review",
    "generation_date": "2026-02-14",
    "review_status": "auto_generated",
    "evidence_count": 70,
    "last_verification": "2026-02-14"
  }
}
