{
  "schema_version": "1.0.0",
  "id": "cuda/torch-cuda-oom-new/torch2-a100",
  "url": "https://deadends.dev/cuda/torch-cuda-oom-new/torch2-a100",
  "error": {
    "signature": "torch.cuda.OutOfMemoryError: CUDA out of memory",
    "regex": "torch\\.cuda\\.OutOfMemoryError|CUDA out of memory",
    "domain": "cuda",
    "category": "oom_error",
    "first_seen": "2023-01-01",
    "last_confirmed": "2026-02-11"
  },
  "environment": {
    "runtime": {
      "name": "cuda",
      "version_range": ">=12.0,<13.0"
    },
    "os": "linux",
    "hardware": {
      "gpu": "A100",
      "vram_gb": 40
    }
  },
  "verdict": {
    "resolvable": "partial",
    "fix_success_rate": 0.85,
    "confidence": 0.9,
    "last_updated": "2026-02-11",
    "summary": "New PyTorch 2.x OOM error type. GPU memory exhausted during training or inference."
  },
  "dead_ends": [
    {
      "action": "Add torch.cuda.empty_cache() in training loop",
      "why_fails": "Only frees cached memory, not memory held by tensors — negligible effect",
      "fail_rate": 0.75,
      "sources": [
        "https://pytorch.org/docs/stable/generated/torch.cuda.empty_cache.html"
      ],
      "condition": ""
    },
    {
      "action": "Set PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb",
      "why_fails": "Only helps with fragmentation, not actual OOM",
      "fail_rate": 0.65,
      "sources": [
        "https://pytorch.org/docs/stable/notes/cuda.html#memory-management"
      ],
      "condition": ""
    }
  ],
  "workarounds": [
    {
      "action": "Reduce batch size — most direct fix for OOM",
      "success_rate": 0.95,
      "sources": [
        "https://pytorch.org/docs/stable/notes/cuda.html"
      ],
      "condition": "",
      "how": ""
    },
    {
      "action": "Use gradient checkpointing to trade compute for memory",
      "success_rate": 0.88,
      "how": "model.gradient_checkpointing_enable()",
      "sources": [
        "https://pytorch.org/docs/stable/checkpoint.html"
      ],
      "condition": ""
    },
    {
      "action": "Use mixed precision training (fp16/bf16) to halve memory",
      "success_rate": 0.9,
      "how": "scaler = torch.amp.GradScaler(); with torch.autocast('cuda'):",
      "sources": [
        "https://pytorch.org/docs/stable/amp.html"
      ],
      "condition": ""
    },
    {
      "action": "Use DeepSpeed ZeRO or FSDP for multi-GPU memory distribution",
      "success_rate": 0.82,
      "sources": [
        "https://pytorch.org/docs/stable/fsdp.html"
      ],
      "condition": "",
      "how": ""
    }
  ],
  "transition_graph": {
    "leads_to": [],
    "preceded_by": [],
    "frequently_confused_with": []
  },
  "metadata": {
    "generated_by": "bulk_generate.py",
    "generation_date": "2026-02-11",
    "review_status": "auto_generated",
    "evidence_count": 50,
    "last_verification": "2026-02-11"
  }
}
