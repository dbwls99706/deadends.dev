{
  "schema_version": "1.0.0",
  "id": "python/cuda-out-of-memory/torch2.1-a100-40gb",
  "url": "https://deadends.dev/python/cuda-out-of-memory/torch2.1-a100-40gb",
  "error": {
    "signature": "RuntimeError: CUDA out of memory. Tried to allocate X GiB",
    "regex": "RuntimeError: CUDA out of memory\\. Tried to allocate \\d+\\.\\d+ [GM]iB",
    "domain": "python",
    "category": "resource_exhaustion",
    "first_seen": "2020-03-15",
    "last_confirmed": "2025-05-20"
  },
  "environment": {
    "runtime": {
      "name": "pytorch",
      "version_range": ">=2.0,<2.3"
    },
    "hardware": {
      "gpu": "A100-40GB",
      "vram_gb": 40
    },
    "os": "linux",
    "python": ">=3.9,<3.13",
    "additional": {}
  },
  "verdict": {
    "resolvable": "partial",
    "fix_success_rate": 0.34,
    "confidence": 0.82,
    "last_updated": "2025-05-20",
    "summary": "In this environment, allocation failures of 2GiB+ are not resolved by batch_size adjustment alone in 66% of cases. gradient_checkpointing shows the highest success rate but depends on model architecture."
  },
  "dead_ends": [
    {
      "action": "Calling torch.cuda.empty_cache()",
      "why_fails": "Does not resolve memory fragmentation. Space occupied by already-allocated tensors is not freed. Ineffective when the root cause is lack of contiguous memory blocks.",
      "fail_rate": 0.89,
      "sources": [
        "https://github.com/pytorch/pytorch/issues/16417"
      ],
      "common_misconception": "Many believe empty_cache 'cleans up' GPU memory, but it only returns cached unused blocks to the allocator — it has no effect on active tensors."
    },
    {
      "action": "Reducing batch_size to 1",
      "why_fails": "If model parameters themselves exceed VRAM, OOM occurs even at batch_size=1. On A100-40GB, 13B+ parameter models require 26GB+ in FP16 for parameters alone.",
      "fail_rate": 0.52,
      "condition": "model_params >= 13B",
      "sources": []
    },
    {
      "action": "Setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True",
      "why_fails": "Introduced in PyTorch 2.1 but does not work in A100 MIG partition environments. Only effective for fragmentation — useless for absolute memory shortage.",
      "fail_rate": 0.61,
      "condition": "A100 MIG mode or absolute memory shortage",
      "sources": [
        "https://pytorch.org/docs/stable/notes/cuda.html"
      ]
    }
  ],
  "workarounds": [
    {
      "action": "Enable gradient_checkpointing",
      "how": "model.gradient_checkpointing_enable()",
      "success_rate": 0.72,
      "tradeoff": "Training speed decreases by 20-30%",
      "condition": "Model must support gradient_checkpointing (most HuggingFace models do)",
      "sources": [
        "https://huggingface.co/docs/transformers/perf_train_gpu_one"
      ]
    },
    {
      "action": "Switch to mixed precision (FP16/BF16)",
      "how": "torch.cuda.amp.autocast() or Trainer(fp16=True)",
      "success_rate": 0.58,
      "tradeoff": "Possible numerical instability in some operations",
      "condition": "Model must be FP16-stable. BF16 is natively supported on A100.",
      "sources": []
    },
    {
      "action": "Apply DeepSpeed ZeRO Stage 2/3",
      "how": "deepspeed --num_gpus=1 train.py --deepspeed ds_config.json",
      "success_rate": 0.85,
      "tradeoff": "Increased configuration complexity; single GPU may require ZeRO-Offload",
      "condition": "DeepSpeed must be installable; NCCL compatibility required",
      "sources": [
        "https://www.deepspeed.ai/tutorials/zero/"
      ]
    }
  ],
  "transition_graph": {
    "leads_to": [
      {
        "error_id": "python/nccl-timeout/torch2.1-multi-gpu",
        "probability": 0.41,
        "condition": "When switching to multi-GPU to avoid OOM",
        "typical_delay": "immediate to minutes"
      },
      {
        "error_id": "python/cuda-device-assert/torch2.1-a100",
        "probability": 0.15,
        "condition": "When forcing computation under memory shortage",
        "typical_delay": "immediate"
      }
    ],
    "preceded_by": [
      {
        "error_id": "python/torch-compile-error/torch2.1",
        "probability": 0.23,
        "condition": "torch.compile() increases memory usage"
      }
    ],
    "frequently_confused_with": [
      {
        "error_id": "python/cuda-device-side-assert/torch2.1-a100",
        "distinction": "OOM is a memory allocation failure; device-side assert is an index/dimension error during computation. Distinguishable by the first line of the error message."
      }
    ]
  },
  "metadata": {
    "generated_by": "deadend-seed-v1",
    "generation_date": "2025-06-01",
    "review_status": "auto_generated",
    "evidence_count": 47,
    "page_views": 0,
    "ai_agent_hits": 0,
    "human_hits": 0,
    "last_verification": "2025-06-01"
  }
}
